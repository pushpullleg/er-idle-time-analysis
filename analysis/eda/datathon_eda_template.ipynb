{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, numpy, matplotlib, seaborn, scipy\n",
    "print(\"\u2713 All packages available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datathon EDA Template\n",
    "\n",
    "**Team Name:** ACM\n",
    "**Date:** 11/06/2024 \n",
    "**Competition:** Alteryx Datathon \n",
    "**Dataset:** Meridian City Hospital ER \n",
    "\n",
    "---\n",
    "\n",
    "## Team Workflow Strategy\n",
    "\n",
    "**Phase 1 (0-15 min): Together**\n",
    "- Run Sections 0 & 1 as a team\n",
    "- Discuss problem context and target variable\n",
    "- Align on objectives\n",
    "\n",
    "**Phase 2 (15-45 min): Parallel Work**\n",
    "- **Member 1:** Sections 2 & 3 (Data quality and univariate analysis)\n",
    "- **Member 2:** Section 4 (Bivariate relationships)\n",
    "- **Member 3:** Section 5 (Multivariate patterns and modeling prep)\n",
    "\n",
    "**Phase 3 (45-60 min): Together**\n",
    "- Share findings (5 min each)\n",
    "- Section 6: Brainstorm features together\n",
    "- Section 7: Plan modeling strategy\n",
    "- Assign next tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize findings dictionary for systematic documentation\n",
    "findings = {\n",
    "    'data_quality_issues': [],\n",
    "    'key_insights': [],\n",
    "    'feature_ideas': [],\n",
    "    'questions_for_team': [],\n",
    "    'next_steps': []\n",
    "}\n",
    "\n",
    "print(\"\u2713 Section 0: Setup & Imports completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Multiple Datasets & Initial Inspection\n",
    "\n",
    "**Team Activity:** Run together and discuss problem context\n",
    "\n",
    "**Dataset:** Meridian City Hospital ER Data (5 related CSV files)\n",
    "\n",
    "**Approach for Multiple Datasets:**\n",
    "1. Load all datasets first\n",
    "2. Understand relationships between datasets\n",
    "3. Perform EDA on each dataset individually\n",
    "4. Analyze relationships across datasets\n",
    "5. Merge/join datasets if needed for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.1 LOAD ALL DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "data_path = 'Meridian_City_Hospital_Data/'\n",
    "\n",
    "# Load all 5 datasets\n",
    "print(\"Loading all datasets...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "# Load each dataset\n",
    "try:\n",
    "    datasets['facility'] = pd.read_csv(data_path + 'Hospital_Facility.csv')\n",
    "    print(\"\u2713 Loaded: Hospital_Facility.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading Hospital_Facility.csv: {e}\")\n",
    "\n",
    "try:\n",
    "    datasets['outcomes'] = pd.read_csv(data_path + 'Hospital_Outcomes.csv')\n",
    "    print(\"\u2713 Loaded: Hospital_Outcomes.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading Hospital_Outcomes.csv: {e}\")\n",
    "\n",
    "try:\n",
    "    datasets['patients'] = pd.read_csv(data_path + 'Hospital_Patients.csv')\n",
    "    print(\"\u2713 Loaded: Hospital_Patients.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading Hospital_Patients.csv: {e}\")\n",
    "\n",
    "try:\n",
    "    datasets['staffing'] = pd.read_csv(data_path + 'Hospital_Staffing_EAST_LOCATION.csv')\n",
    "    print(\"\u2713 Loaded: Hospital_Staffing_EAST_LOCATION.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading Hospital_Staffing_EAST_LOCATION.csv: {e}\")\n",
    "\n",
    "try:\n",
    "    datasets['visits'] = pd.read_csv(data_path + 'Hospital_Visits.csv')\n",
    "    print(\"\u2713 Loaded: Hospital_Visits.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error loading Hospital_Visits.csv: {e}\")\n",
    "\n",
    "print(f\"\\n\u2713 Successfully loaded {len(datasets)} datasets\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1.2 DATASET OVERVIEW - ALL DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET OVERVIEW - ALL FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dataset_summary = []\n",
    "for name, df in datasets.items():\n",
    "    dataset_summary.append({\n",
    "        'Dataset': name,\n",
    "        'Rows': df.shape[0],\n",
    "        'Columns': df.shape[1],\n",
    "        'Memory (MB)': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'Numerical Cols': len(df.select_dtypes(include=[np.number]).columns),\n",
    "        'Categorical Cols': len(df.select_dtypes(include=['object', 'category']).columns),\n",
    "        'Datetime Cols': len(df.select_dtypes(include=['datetime64']).columns)\n",
    "    })\n",
    "    \n",
    "    # Document initial observations\n",
    "    findings['key_insights'].append(\n",
    "        f\"{name}: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns \"\n",
    "        f\"({df.memory_usage(deep=True).sum() / 1024**2:.2f} MB)\"\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(dataset_summary)\n",
    "display(summary_df)\n",
    "\n",
    "# ============================================================================\n",
    "# 1.3 COLUMN INFORMATION - ALL DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COLUMN INFORMATION - ALL DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name.upper()} ({len(df.columns)} columns):\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        dtype = str(df[col].dtype)\n",
    "        print(f\"  {i:2d}. {col:<30} ({dtype})\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1.4 FIRST FEW ROWS OF EACH DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIRST 3 ROWS OF EACH DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{name.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    display(df.head(3))\n",
    "    print(f\"\\nShape: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1.5 IDENTIFY COMMON COLUMNS (POTENTIAL JOIN KEYS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IDENTIFYING COMMON COLUMNS (POTENTIAL JOIN KEYS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all column names from all datasets\n",
    "all_columns = {}\n",
    "for name, df in datasets.items():\n",
    "    all_columns[name] = set(df.columns)\n",
    "\n",
    "# Find common columns\n",
    "print(\"\\nCommon columns across datasets:\")\n",
    "common_columns = {}\n",
    "dataset_names = list(datasets.keys())\n",
    "\n",
    "for i, name1 in enumerate(dataset_names):\n",
    "    for name2 in dataset_names[i+1:]:\n",
    "        common = all_columns[name1] & all_columns[name2]\n",
    "        if common:\n",
    "            key = f\"{name1} <-> {name2}\"\n",
    "            common_columns[key] = common\n",
    "            print(f\"\\n{key}:\")\n",
    "            for col in sorted(common):\n",
    "                print(f\"  \u2022 {col}\")\n",
    "                findings['key_insights'].append(f\"Common column '{col}' found in {name1} and {name2} (potential join key)\")\n",
    "\n",
    "if not common_columns:\n",
    "    print(\"  No common columns found. Datasets may need to be joined differently.\")\n",
    "    findings['questions_for_team'].append(\"How are these datasets related? What are the join keys?\")\n",
    "\n",
    "print(\"\\n\u2713 Section 1: Load Multiple Datasets & Initial Inspection completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.5: Dataset Relationships & Selection Helper\n",
    "\n",
    "**Team Activity:** Understand how datasets relate before deep analysis\n",
    "\n",
    "**Strategy:**\n",
    "- Analyze relationships between datasets\n",
    "- Identify join keys\n",
    "- Select which dataset(s) to analyze in detail\n",
    "- Decide if you need merged datasets for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.5.1 DATASET RELATIONSHIP ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET RELATIONSHIP ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for potential ID columns that might link datasets\n",
    "print(\"\\nAnalyzing potential relationships...\")\n",
    "\n",
    "# Common ID patterns\n",
    "id_patterns = ['id', 'ID', 'Id', '_id', 'key', 'Key', 'code', 'Code', \n",
    "               'patient', 'Patient', 'visit', 'Visit', 'facility', 'Facility',\n",
    "               'hospital', 'Hospital', 'staff', 'Staff']\n",
    "\n",
    "print(\"\\nPotential ID/Key columns in each dataset:\")\n",
    "for name, df in datasets.items():\n",
    "    potential_ids = [col for col in df.columns if any(pattern in col for pattern in id_patterns)]\n",
    "    if potential_ids:\n",
    "        print(f\"\\n{name.upper()}:\")\n",
    "        for col in potential_ids:\n",
    "            unique_count = df[col].nunique()\n",
    "            total_count = len(df)\n",
    "            print(f\"  \u2022 {col}: {unique_count:,} unique values / {total_count:,} total\")\n",
    "            if unique_count == total_count:\n",
    "                print(f\"    \u2713 Primary key candidate (all unique)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1.5.2 CREATE MERGED DATASET (OPTIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET MERGING GUIDANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\ud83d\udca1 TIP: Before merging, identify the correct join keys.\")\n",
    "print(\"   Common relationships in hospital data:\")\n",
    "print(\"   - Patients <-> Visits (Patient ID)\")\n",
    "print(\"   - Visits <-> Outcomes (Visit ID)\")\n",
    "print(\"   - Visits <-> Facility (Facility ID)\")\n",
    "print(\"   - Staffing <-> Facility (Location/Facility ID)\")\n",
    "\n",
    "# Example merge (uncomment and modify based on your actual join keys):\n",
    "# merged_df = datasets['patients'].merge(datasets['visits'], on='PatientID', how='inner')\n",
    "# print(f\"\\nMerged patients + visits: {merged_df.shape[0]:,} rows \u00d7 {merged_df.shape[1]} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1.5.3 DATASET SELECTION HELPER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SELECT DATASET FOR DETAILED ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nAvailable datasets:\")\n",
    "for i, name in enumerate(datasets.keys(), 1):\n",
    "    df = datasets[name]\n",
    "    print(f\"  {i}. {name}: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 INSTRUCTIONS:\")\n",
    "print(\"   1. Choose which dataset(s) to analyze in detail\")\n",
    "print(\"   2. Set 'current_dataset' to the dataset name below\")\n",
    "print(\"   3. Sections 2-5 will analyze the selected dataset\")\n",
    "print(\"   4. Repeat for other datasets as needed\")\n",
    "\n",
    "# Set the dataset you want to analyze in detail\n",
    "# Options: 'facility', 'outcomes', 'patients', 'staffing', 'visits'\n",
    "# Or create a merged dataset above and use that\n",
    "\n",
    "current_dataset_name = 'visits'  # CHANGE THIS to analyze a different dataset\n",
    "current_dataset = datasets[current_dataset_name].copy()\n",
    "\n",
    "print(f\"\\n\u2713 Selected dataset for detailed analysis: {current_dataset_name}\")\n",
    "print(f\"  Shape: {current_dataset.shape[0]:,} rows \u00d7 {current_dataset.shape[1]} columns\")\n",
    "\n",
    "# Create alias 'df' for compatibility with rest of the template\n",
    "df = current_dataset.copy()\n",
    "\n",
    "print(\"\\n\u2713 Section 1.5: Dataset Relationships & Selection completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Comparison: Data Quality Across All Datasets (Optional)\n",
    "\n",
    "Run this cell to quickly compare data quality metrics across all datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QUICK DATA QUALITY COMPARISON - ALL DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUICK DATA QUALITY COMPARISON - ALL DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "quality_comparison = []\n",
    "\n",
    "for name, df_temp in datasets.items():\n",
    "    missing_pct = (df_temp.isnull().sum().sum() / (df_temp.shape[0] * df_temp.shape[1])) * 100\n",
    "    duplicate_count = df_temp.duplicated().sum()\n",
    "    duplicate_pct = (duplicate_count / len(df_temp)) * 100 if len(df_temp) > 0 else 0\n",
    "    \n",
    "    quality_comparison.append({\n",
    "        'Dataset': name,\n",
    "        'Rows': df_temp.shape[0],\n",
    "        'Columns': df_temp.shape[1],\n",
    "        'Missing %': f\"{missing_pct:.2f}%\",\n",
    "        'Duplicates': duplicate_count,\n",
    "        'Duplicate %': f\"{duplicate_pct:.2f}%\",\n",
    "        'Memory (MB)': f\"{df_temp.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
    "    })\n",
    "\n",
    "quality_df = pd.DataFrame(quality_comparison)\n",
    "display(quality_df)\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Use this to prioritize which datasets need more attention during detailed analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Quality Assessment\n",
    "\n",
    "**Assigned to: Member 1**  \n",
    "**Time: 15-30 minutes**\n",
    "\n",
    "**Note:** This section analyzes the currently selected dataset (`df`). \n",
    "To analyze a different dataset, go back to Section 1.5 and change `current_dataset_name`.\n",
    "\n",
    "**For Multiple Datasets:** You can modify this section to loop through all datasets if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"DATA QUALITY ASSESSMENT - {current_dataset_name.upper()}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Analyzing dataset: {current_dataset_name}\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")\n",
    "\n",
    "# 2.1 Missing Values Analysis\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2.1 MISSING VALUES ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing Count': missing_data.values,\n",
    "    'Missing Percentage': missing_percent.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Visualize missing values\n",
    "    plt.figure(figsize=(12, max(6, len(missing_df) * 0.5)))\n",
    "    sns.barplot(data=missing_df, y='Column', x='Missing Percentage', palette='Reds_r')\n",
    "    plt.title('Missing Values by Column', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Missing Percentage (%)', fontsize=12)\n",
    "    plt.ylabel('Column', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Document findings\n",
    "    for _, row in missing_df.iterrows():\n",
    "        findings['data_quality_issues'].append(\n",
    "            f\"{row['Column']}: {row['Missing Count']:,} missing values ({row['Missing Percentage']:.2f}%)\"\n",
    "        )\n",
    "else:\n",
    "    print(\"\u2713 No missing values found in the dataset\")\n",
    "    findings['key_insights'].append(\"No missing values detected in the dataset\")\n",
    "\n",
    "# 2.2 Duplicate Rows\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2.2 DUPLICATE ROWS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicate_count:,} ({duplicate_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(\"\\nSample duplicate rows:\")\n",
    "    display(df[df.duplicated(keep=False)].head(10))\n",
    "    findings['data_quality_issues'].append(f\"{duplicate_count:,} duplicate rows found ({duplicate_count/len(df)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"\u2713 No duplicate rows found\")\n",
    "    findings['key_insights'].append(\"No duplicate rows detected\")\n",
    "\n",
    "# 2.3 Data Type Verification\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2.3 DATA TYPE VERIFICATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check for columns that might be incorrectly typed\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns: {len(numeric_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"Datetime columns: {len(datetime_cols)}\")\n",
    "\n",
    "# Check for mixed types in object columns\n",
    "print(\"\\nChecking for mixed types in object columns...\")\n",
    "mixed_type_cols = []\n",
    "for col in categorical_cols:\n",
    "    # Try to convert to numeric and see if there are any numeric values\n",
    "    numeric_vals = pd.to_numeric(df[col], errors='coerce')\n",
    "    if numeric_vals.notna().sum() > 0 and numeric_vals.notna().sum() < len(df):\n",
    "        mixed_type_cols.append(col)\n",
    "        findings['data_quality_issues'].append(f\"{col}: Mixed data types detected (numeric and non-numeric)\")\n",
    "\n",
    "if mixed_type_cols:\n",
    "    print(f\"\u26a0 Found {len(mixed_type_cols)} columns with mixed types: {mixed_type_cols}\")\n",
    "else:\n",
    "    print(\"\u2713 No mixed type columns detected\")\n",
    "\n",
    "# 2.4 Constant Columns\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2.4 CONSTANT COLUMNS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "constant_cols = []\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() <= 1:\n",
    "        constant_cols.append(col)\n",
    "        findings['data_quality_issues'].append(f\"{col}: Constant column (only {df[col].nunique()} unique value)\")\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"\u26a0 Found {len(constant_cols)} constant columns: {constant_cols}\")\n",
    "else:\n",
    "    print(\"\u2713 No constant columns found\")\n",
    "\n",
    "# 2.5 Data Quality Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal columns: {df.shape[1]}\")\n",
    "print(f\"Columns with missing values: {len(missing_df) if len(missing_df) > 0 else 0}\")\n",
    "print(f\"Duplicate rows: {duplicate_count:,}\")\n",
    "print(f\"Mixed type columns: {len(mixed_type_cols)}\")\n",
    "print(f\"Constant columns: {len(constant_cols)}\")\n",
    "print(f\"Total data quality issues: {len(findings['data_quality_issues'])}\")\n",
    "\n",
    "print(\"\\n\u2713 Section 2: Data Quality Assessment completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Univariate Analysis\n",
    "\n",
    "**Assigned to: Member 1**  \n",
    "**Time: 15-30 minutes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"UNIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 3.1 Separate Numerical and Categorical Columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "# 3.2 Descriptive Statistics for Numerical Variables\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"3.2 DESCRIPTIVE STATISTICS (NUMERICAL)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    desc_stats = df[numeric_cols].describe().T\n",
    "    desc_stats['skewness'] = df[numeric_cols].skew()\n",
    "    desc_stats['kurtosis'] = df[numeric_cols].kurtosis()\n",
    "    desc_stats['missing_count'] = df[numeric_cols].isnull().sum()\n",
    "    desc_stats['missing_pct'] = (desc_stats['missing_count'] / len(df)) * 100\n",
    "    \n",
    "    display(desc_stats)\n",
    "    \n",
    "    # Document insights about distributions\n",
    "    for col in numeric_cols:\n",
    "        skew_val = desc_stats.loc[col, 'skewness']\n",
    "        if abs(skew_val) > 1:\n",
    "            findings['key_insights'].append(\n",
    "                f\"{col}: Highly {'right' if skew_val > 0 else 'left'}-skewed distribution (skewness={skew_val:.2f})\"\n",
    "            )\n",
    "\n",
    "# 3.3 Outlier Detection (IQR Method)\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"3.3 OUTLIER DETECTION (IQR METHOD)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    outlier_summary = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_pct = (outlier_count / len(df)) * 100\n",
    "        \n",
    "        if outlier_count > 0:\n",
    "            outlier_summary.append({\n",
    "                'Column': col,\n",
    "                'Outlier Count': outlier_count,\n",
    "                'Outlier Percentage': outlier_pct,\n",
    "                'Lower Bound': lower_bound,\n",
    "                'Upper Bound': upper_bound\n",
    "            })\n",
    "            \n",
    "            if outlier_pct > 5:  # Flag if more than 5% outliers\n",
    "                findings['data_quality_issues'].append(\n",
    "                    f\"{col}: {outlier_count:,} outliers ({outlier_pct:.2f}%) detected using IQR method\"\n",
    "                )\n",
    "    \n",
    "    if outlier_summary:\n",
    "        outlier_df = pd.DataFrame(outlier_summary)\n",
    "        display(outlier_df)\n",
    "    else:\n",
    "        print(\"\u2713 No significant outliers detected using IQR method\")\n",
    "\n",
    "# 3.4 Categorical Variable Analysis\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"3.4 CATEGORICAL VARIABLE CARDINALITY\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    cat_summary = []\n",
    "    for col in categorical_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        cat_summary.append({\n",
    "            'Column': col,\n",
    "            'Unique Values': unique_count,\n",
    "            'Most Frequent': df[col].mode()[0] if len(df[col].mode()) > 0 else 'N/A',\n",
    "            'Most Frequent Count': df[col].value_counts().iloc[0] if unique_count > 0 else 0,\n",
    "            'Most Frequent %': (df[col].value_counts().iloc[0] / len(df) * 100) if unique_count > 0 else 0\n",
    "        })\n",
    "        \n",
    "        # Document high cardinality\n",
    "        if unique_count > 50:\n",
    "            findings['key_insights'].append(\n",
    "                f\"{col}: High cardinality categorical variable ({unique_count} unique values)\"\n",
    "            )\n",
    "    \n",
    "    cat_df = pd.DataFrame(cat_summary)\n",
    "    display(cat_df)\n",
    "\n",
    "# 3.5 Distribution Visualizations - Numerical\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"3.5 DISTRIBUTION VISUALIZATIONS (NUMERICAL)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_cols = min(3, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten() if len(numeric_cols) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        ax = axes[idx]\n",
    "        df[col].hist(bins=50, ax=ax, edgecolor='black', alpha=0.7)\n",
    "        ax.set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(col, fontsize=10)\n",
    "        ax.set_ylabel('Frequency', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(numeric_cols), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3.6 Value Counts - Categorical\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"3.6 VALUE COUNTS (CATEGORICAL - TOP 10)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for col in categorical_cols[:5]:  # Limit to first 5 to avoid too much output\n",
    "        print(f\"\\n{col}:\")\n",
    "        value_counts = df[col].value_counts().head(10)\n",
    "        display(value_counts.to_frame('Count'))\n",
    "        \n",
    "        # Visualize top categories\n",
    "        if df[col].nunique() <= 20:  # Only plot if reasonable number of categories\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts.plot(kind='bar')\n",
    "            plt.title(f'Value Counts: {col}', fontsize=12, fontweight='bold')\n",
    "            plt.xlabel(col, fontsize=10)\n",
    "            plt.ylabel('Count', fontsize=10)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Section 3: Univariate Analysis completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Bivariate Analysis\n",
    "\n",
    "**Assigned to: Member 2**  \n",
    "**Time: 15-30 minutes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"BIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 4.1 Correlation Matrix\n",
    "if len(numeric_cols) > 1:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"4.1 CORRELATION MATRIX\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Display correlation matrix\n",
    "    display(correlation_matrix)\n",
    "    \n",
    "    # Visualize correlation heatmap\n",
    "    plt.figure(figsize=(max(10, len(numeric_cols)), max(8, len(numeric_cols))))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Matrix (Numerical Features)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 4.2 High Correlation Detection\n",
    "if len(numeric_cols) > 1:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"4.2 HIGH CORRELATION PAIRS (|r| > 0.7)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            col1 = correlation_matrix.columns[i]\n",
    "            col2 = correlation_matrix.columns[j]\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            \n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr_pairs.append({\n",
    "                    'Feature 1': col1,\n",
    "                    'Feature 2': col2,\n",
    "                    'Correlation': corr_val\n",
    "                })\n",
    "                findings['key_insights'].append(\n",
    "                    f\"High correlation between {col1} and {col2}: {corr_val:.3f}\"\n",
    "                )\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        high_corr_df = pd.DataFrame(high_corr_pairs)\n",
    "        display(high_corr_df.sort_values('Correlation', key=abs, ascending=False))\n",
    "    else:\n",
    "        print(\"\u2713 No highly correlated pairs found (|r| > 0.7)\")\n",
    "\n",
    "# 4.3 Target Variable Relationship Analysis\n",
    "# Uncomment and modify if you have a target variable\n",
    "\n",
    "target_col = 'Visit ID'  # SET YOUR TARGET COLUMN HERE\n",
    "\n",
    "if target_col in df.columns:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"4.3 TARGET VARIABLE ANALYSIS: {target_col}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # If target is numerical\n",
    "    if df[target_col].dtype in [np.number]:\n",
    "        # Correlation with target\n",
    "        target_corr = df[numeric_cols].corrwith(df[target_col]).sort_values(key=abs, ascending=False)\n",
    "        print(\"\\nCorrelation with target:\")\n",
    "        display(target_corr.to_frame('Correlation'))\n",
    "        \n",
    "        # Top correlated features\n",
    "        top_features = target_corr.abs().nlargest(10).index.tolist()\n",
    "        findings['key_insights'].append(f\"Top features correlated with {target_col}: {top_features[:5]}\")\n",
    "        \n",
    "        # Scatter plots for top features\n",
    "        n_top = min(6, len(top_features))\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, feature in enumerate(top_features[:n_top]):\n",
    "            ax = axes[idx]\n",
    "            ax.scatter(df[feature], df[target_col], alpha=0.5, s=20)\n",
    "            ax.set_xlabel(feature, fontsize=10)\n",
    "            ax.set_ylabel(target_col, fontsize=10)\n",
    "            ax.set_title(f'{feature} vs {target_col}\\n(r={target_corr[feature]:.3f})', fontsize=11)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # If target is categorical\n",
    "    elif df[target_col].dtype in ['object', 'category']:\n",
    "        # Class distribution\n",
    "        print(\"\\nTarget class distribution:\")\n",
    "        target_dist = df[target_col].value_counts()\n",
    "        display(target_dist.to_frame('Count'))\n",
    "        \n",
    "        # Visualize class distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        target_dist.plot(kind='bar')\n",
    "        plt.title(f'Distribution of {target_col}', fontsize=12, fontweight='bold')\n",
    "        plt.xlabel(target_col, fontsize=10)\n",
    "        plt.ylabel('Count', fontsize=10)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Box plots for numerical features by target class\n",
    "        if len(numeric_cols) > 0:\n",
    "            n_features = min(6, len(numeric_cols))\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for idx, feature in enumerate(numeric_cols[:n_features]):\n",
    "                ax = axes[idx]\n",
    "                df.boxplot(column=feature, by=target_col, ax=ax)\n",
    "                ax.set_title(f'{feature} by {target_col}', fontsize=11)\n",
    "                ax.set_xlabel(target_col, fontsize=10)\n",
    "                ax.set_ylabel(feature, fontsize=10)\n",
    "                plt.suptitle('')  # Remove default title\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(f\"\\n\u26a0 Target column '{target_col}' not found in dataset\")\n",
    "\n",
    "print(\"\\n\u2713 Section 4: Bivariate Analysis completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Multivariate Analysis & Patterns\n",
    "\n",
    "**Assigned to: Member 3**  \n",
    "**Time: 15-30 minutes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MULTIVARIATE ANALYSIS & PATTERNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 5.1 Pairplot for Pattern Detection\n",
    "if len(numeric_cols) > 1 and len(numeric_cols) <= 8:  # Limit to avoid too many plots\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"5.1 PAIRPLOT (PATTERN DETECTION)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nGenerating pairplot (this may take a moment)...\")\n",
    "    \n",
    "    # Sample data if too large for pairplot\n",
    "    sample_size = min(1000, len(df))\n",
    "    df_sample = df[numeric_cols].sample(n=sample_size, random_state=42) if len(df) > 1000 else df[numeric_cols]\n",
    "    \n",
    "    # Create pairplot\n",
    "    sns.pairplot(df_sample, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 20})\n",
    "    plt.suptitle('Pairplot of Numerical Features', y=1.02, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    findings['key_insights'].append(\"Pairplot generated to identify multivariate patterns and clusters\")\n",
    "elif len(numeric_cols) > 8:\n",
    "    print(\"\\n\u26a0 Too many numerical columns for pairplot. Consider selecting key features.\")\n",
    "    findings['questions_for_team'].append(\"Which numerical features should we focus on for multivariate analysis?\")\n",
    "\n",
    "# 5.2 Class Imbalance Detection (if target exists)\n",
    "# Uncomment if you have a target variable\n",
    "# if 'target_col' in locals() and target_col in df.columns:\n",
    "#     if df[target_col].dtype in ['object', 'category']:\n",
    "#         print(\"\\n\" + \"-\" * 80)\n",
    "#         print(\"5.2 CLASS IMBALANCE DETECTION\")\n",
    "#         print(\"-\" * 80)\n",
    "#         \n",
    "#         class_counts = df[target_col].value_counts()\n",
    "#         class_proportions = class_counts / len(df)\n",
    "#         \n",
    "#         print(\"\\nClass distribution:\")\n",
    "#         display(class_proportions.to_frame('Proportion'))\n",
    "#         \n",
    "#         # Check for imbalance (threshold: any class < 10%)\n",
    "#         min_proportion = class_proportions.min()\n",
    "#         if min_proportion < 0.1:\n",
    "#             findings['data_quality_issues'].append(\n",
    "#                 f\"Class imbalance detected: Minority class represents {min_proportion*100:.2f}% of data\"\n",
    "#             )\n",
    "#             findings['next_steps'].append(\"Consider using class balancing techniques (SMOTE, undersampling, etc.)\")\n",
    "\n",
    "# 5.3 Time-based Pattern Analysis\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "if len(datetime_cols) > 0:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"5.3 TIME-BASED PATTERN ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for col in datetime_cols[:2]:  # Limit to first 2 datetime columns\n",
    "        print(f\"\\nAnalyzing {col}...\")\n",
    "        \n",
    "        # Extract time components\n",
    "        df[f'{col}_year'] = pd.to_datetime(df[col]).dt.year\n",
    "        df[f'{col}_month'] = pd.to_datetime(df[col]).dt.month\n",
    "        df[f'{col}_day'] = pd.to_datetime(df[col]).dt.day\n",
    "        df[f'{col}_dayofweek'] = pd.to_datetime(df[col]).dt.dayofweek\n",
    "        \n",
    "        # Time series plot if we have a numerical target or feature\n",
    "        if len(numeric_cols) > 0:\n",
    "            # Plot first numerical column over time\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            time_series = df.groupby(pd.to_datetime(df[col]).dt.date)[numeric_cols[0]].mean()\n",
    "            time_series.plot()\n",
    "            plt.title(f'{numeric_cols[0]} Over Time ({col})', fontsize=12, fontweight='bold')\n",
    "            plt.xlabel('Date', fontsize=10)\n",
    "            plt.ylabel(numeric_cols[0], fontsize=10)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            findings['feature_ideas'].append(f\"Extract time features from {col}: year, month, day, dayofweek, hour, etc.\")\n",
    "else:\n",
    "    print(\"\\n\u2713 No datetime columns found for time-based analysis\")\n",
    "\n",
    "# 5.4 Cluster and Grouping Identification\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"5.4 CLUSTER IDENTIFICATION PREPARATION\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nReview pairplot and correlation matrix for potential clusters or groups.\")\n",
    "print(\"Consider using dimensionality reduction (PCA, t-SNE) if needed.\")\n",
    "\n",
    "if len(numeric_cols) > 2:\n",
    "    findings['questions_for_team'].append(\"Should we apply dimensionality reduction techniques (PCA, t-SNE) for visualization?\")\n",
    "\n",
    "# 5.5 Preparation for Modeling Insights\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"5.5 MODELING PREPARATION INSIGHTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "modeling_insights = []\n",
    "\n",
    "# Feature count\n",
    "modeling_insights.append(f\"Total features available: {df.shape[1]}\")\n",
    "modeling_insights.append(f\"Numerical features: {len(numeric_cols)}\")\n",
    "modeling_insights.append(f\"Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "# Data size\n",
    "if df.shape[0] < 1000:\n",
    "    modeling_insights.append(\"Small dataset: Consider simpler models or data augmentation\")\n",
    "elif df.shape[0] > 100000:\n",
    "    modeling_insights.append(\"Large dataset: Can support complex models, consider sampling for faster iteration\")\n",
    "\n",
    "# Missing data impact\n",
    "if len(findings['data_quality_issues']) > 0:\n",
    "    modeling_insights.append(f\"Data quality issues to address: {len(findings['data_quality_issues'])}\")\n",
    "\n",
    "for insight in modeling_insights:\n",
    "    print(f\"\u2022 {insight}\")\n",
    "    findings['next_steps'].append(insight)\n",
    "\n",
    "print(\"\\n\u2713 Section 5: Multivariate Analysis & Patterns completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Feature Engineering Ideas\n",
    "\n",
    "**Team Activity:** Brainstorm together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING IDEAS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nUse this section to brainstorm and document feature engineering ideas.\")\n",
    "print(\"Add your ideas to the findings dictionary as you discuss.\")\n",
    "\n",
    "# 6.1 Numerical Feature Transformations\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"6.1 NUMERICAL FEATURE TRANSFORMATIONS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numerical_transforms = [\n",
    "    \"Polynomial features (x\u00b2, x\u00b3) for non-linear relationships\",\n",
    "    \"Binning/bucketing for continuous variables\",\n",
    "    \"Log/Box-Cox transformations for skewed distributions\",\n",
    "    \"Standardization/Normalization (StandardScaler, MinMaxScaler)\",\n",
    "    \"Robust scaling for outlier-resistant normalization\",\n",
    "    \"Power transforms (square root, cube root)\"\n",
    "]\n",
    "\n",
    "print(\"\\nPotential transformations:\")\n",
    "for transform in numerical_transforms:\n",
    "    print(f\"  \u2022 {transform}\")\n",
    "\n",
    "# Document specific ideas based on your data\n",
    "for col in numeric_cols[:5]:  # Review first 5 numerical columns\n",
    "    skew_val = df[col].skew()\n",
    "    if abs(skew_val) > 1:\n",
    "        findings['feature_ideas'].append(f\"Apply log/Box-Cox transform to {col} (skewness={skew_val:.2f})\")\n",
    "\n",
    "# 6.2 Categorical Encoding Strategies\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"6.2 CATEGORICAL ENCODING STRATEGIES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "categorical_encodings = [\n",
    "    \"One-hot encoding for low cardinality (< 10 unique values)\",\n",
    "    \"Label encoding for ordinal categories\",\n",
    "    \"Target encoding for high cardinality categories\",\n",
    "    \"Frequency encoding (replace with value counts)\",\n",
    "    \"Binary encoding for very high cardinality\",\n",
    "    \"Embedding for deep learning models\"\n",
    "]\n",
    "\n",
    "print(\"\\nPotential encoding strategies:\")\n",
    "for encoding in categorical_encodings:\n",
    "    print(f\"  \u2022 {encoding}\")\n",
    "\n",
    "# Document specific ideas based on your data\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    if unique_count < 10:\n",
    "        findings['feature_ideas'].append(f\"One-hot encode {col} ({unique_count} categories)\")\n",
    "    elif unique_count > 50:\n",
    "        findings['feature_ideas'].append(f\"Consider target/frequency encoding for {col} (high cardinality: {unique_count})\")\n",
    "\n",
    "# 6.3 Datetime Feature Extraction\n",
    "if len(datetime_cols) > 0:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"6.3 DATETIME FEATURE EXTRACTION\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    datetime_features = [\n",
    "        \"Extract: year, month, day, hour, minute, second\",\n",
    "        \"Extract: day of week, day of year, week of year\",\n",
    "        \"Extract: is_weekend, is_month_start, is_month_end\",\n",
    "        \"Extract: quarter, semester\",\n",
    "        \"Time since reference date\",\n",
    "        \"Cyclical encoding (sin/cos) for periodic features\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nPotential datetime features:\")\n",
    "    for feature in datetime_features:\n",
    "        print(f\"  \u2022 {feature}\")\n",
    "    \n",
    "    for col in datetime_cols:\n",
    "        findings['feature_ideas'].append(f\"Extract time components from {col}: year, month, day, hour, dayofweek, etc.\")\n",
    "\n",
    "# 6.4 Domain-Specific Features\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"6.4 DOMAIN-SPECIFIC FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nDiscuss domain knowledge and create relevant features.\")\n",
    "print(\"Examples:\")\n",
    "print(\"  \u2022 Ratio features (e.g., price per unit, density)\")\n",
    "print(\"  \u2022 Interaction features (e.g., product of two features)\")\n",
    "print(\"  \u2022 Aggregate features (e.g., mean, max, min by group)\")\n",
    "print(\"  \u2022 Distance/Similarity features\")\n",
    "print(\"  \u2022 Text features (if applicable): word count, sentiment, etc.\")\n",
    "\n",
    "# 6.5 Aggregate and Rolling Window Features\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"6.5 AGGREGATE & ROLLING WINDOW FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "aggregate_features = [\n",
    "    \"Group-based aggregations (mean, median, std, min, max, count)\",\n",
    "    \"Rolling window statistics (moving average, rolling std)\",\n",
    "    \"Lag features (previous values)\",\n",
    "    \"Cumulative statistics\",\n",
    "    \"Rank-based features\"\n",
    "]\n",
    "\n",
    "print(\"\\nPotential aggregate features:\")\n",
    "for feature in aggregate_features:\n",
    "    print(f\"  \u2022 {feature}\")\n",
    "\n",
    "print(\"\\n\u2713 Section 6: Feature Engineering Ideas completed\")\n",
    "print(\"\\n\ud83d\udca1 TIP: Document your specific feature engineering ideas in the findings dictionary above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Summary & Action Items\n",
    "\n",
    "**Team Activity:** Review together and plan next steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY & ACTION ITEMS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 7.1 Data Quality Issues Summary\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"7.1 DATA QUALITY ISSUES\")\n",
    "print(\"-\" * 80)\n",
    "if len(findings['data_quality_issues']) > 0:\n",
    "    for i, issue in enumerate(findings['data_quality_issues'], 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "else:\n",
    "    print(\"  \u2713 No major data quality issues identified\")\n",
    "\n",
    "# 7.2 Key Insights Summary\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"7.2 KEY INSIGHTS\")\n",
    "print(\"-\" * 80)\n",
    "if len(findings['key_insights']) > 0:\n",
    "    for i, insight in enumerate(findings['key_insights'], 1):\n",
    "        print(f\"  {i}. {insight}\")\n",
    "else:\n",
    "    print(\"  (No insights documented yet)\")\n",
    "\n",
    "# 7.3 Feature Engineering Ideas Summary\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"7.3 FEATURE ENGINEERING IDEAS\")\n",
    "print(\"-\" * 80)\n",
    "if len(findings['feature_ideas']) > 0:\n",
    "    for i, idea in enumerate(findings['feature_ideas'], 1):\n",
    "        print(f\"  {i}. {idea}\")\n",
    "else:\n",
    "    print(\"  (No feature ideas documented yet)\")\n",
    "\n",
    "# 7.4 Questions for Team Discussion\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"7.4 QUESTIONS FOR TEAM DISCUSSION\")\n",
    "print(\"-\" * 80)\n",
    "if len(findings['questions_for_team']) > 0:\n",
    "    for i, question in enumerate(findings['questions_for_team'], 1):\n",
    "        print(f\"  {i}. {question}\")\n",
    "else:\n",
    "    print(\"  (No questions documented yet)\")\n",
    "\n",
    "# 7.5 Next Steps\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"7.5 NEXT STEPS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "next_steps_default = [\n",
    "    \"Address data quality issues (missing values, duplicates, outliers)\",\n",
    "    \"Implement feature engineering based on insights\",\n",
    "    \"Split data into train/validation/test sets\",\n",
    "    \"Select baseline models to try\",\n",
    "    \"Set up cross-validation strategy\",\n",
    "    \"Define evaluation metrics\",\n",
    "    \"Create modeling pipeline\"\n",
    "]\n",
    "\n",
    "all_next_steps = findings['next_steps'] + next_steps_default\n",
    "for i, step in enumerate(all_next_steps, 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "\n",
    "# 7.6 Export Findings to Text File\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"7.6 EXPORTING FINDINGS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "export_content = f\"\"\"\n",
    "EDA FINDINGS REPORT\n",
    "===================\n",
    "Generated: {timestamp}\n",
    "Dataset: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\n",
    "\n",
    "DATA QUALITY ISSUES\n",
    "-------------------\n",
    "\"\"\"\n",
    "\n",
    "if len(findings['data_quality_issues']) > 0:\n",
    "    for i, issue in enumerate(findings['data_quality_issues'], 1):\n",
    "        export_content += f\"{i}. {issue}\\n\"\n",
    "else:\n",
    "    export_content += \"No major data quality issues identified.\\n\"\n",
    "\n",
    "export_content += f\"\\nKEY INSIGHTS\\n\"\n",
    "export_content += f\"{'=' * 20}\\n\"\n",
    "if len(findings['key_insights']) > 0:\n",
    "    for i, insight in enumerate(findings['key_insights'], 1):\n",
    "        export_content += f\"{i}. {insight}\\n\"\n",
    "else:\n",
    "    export_content += \"No insights documented.\\n\"\n",
    "\n",
    "export_content += f\"\\nFEATURE ENGINEERING IDEAS\\n\"\n",
    "export_content += f\"{'=' * 20}\\n\"\n",
    "if len(findings['feature_ideas']) > 0:\n",
    "    for i, idea in enumerate(findings['feature_ideas'], 1):\n",
    "        export_content += f\"{i}. {idea}\\n\"\n",
    "else:\n",
    "    export_content += \"No feature ideas documented.\\n\"\n",
    "\n",
    "export_content += f\"\\nQUESTIONS FOR TEAM DISCUSSION\\n\"\n",
    "export_content += f\"{'=' * 20}\\n\"\n",
    "if len(findings['questions_for_team']) > 0:\n",
    "    for i, question in enumerate(findings['questions_for_team'], 1):\n",
    "        export_content += f\"{i}. {question}\\n\"\n",
    "else:\n",
    "    export_content += \"No questions documented.\\n\"\n",
    "\n",
    "export_content += f\"\\nNEXT STEPS\\n\"\n",
    "export_content += f\"{'=' * 20}\\n\"\n",
    "for i, step in enumerate(all_next_steps, 1):\n",
    "    export_content += f\"{i}. {step}\\n\"\n",
    "\n",
    "# Write to file\n",
    "output_file = 'eda_findings.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(export_content)\n",
    "\n",
    "print(f\"\\n\u2713 Findings exported to: {output_file}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EDA COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nNext: Review findings with team and proceed to feature engineering and modeling.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}